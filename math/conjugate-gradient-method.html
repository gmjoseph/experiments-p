<html>
    <body style="background-color: black;">
        <canvas id="canvas" width="500px" height="500px"></canvas>
    </body>
    <script src="../lib/gl-matrix.js"></script>
    <script>
        const canvas = document.getElementById('canvas');
        // https://en.wikipedia.org/wiki/Conjugate_gradient_method

        // goal:
        // solve Ax = b
        // where A is a symmetric n x n matrix (i.e. A transpose = A) 
        // and x is a vector giving a solution to Ax = b.
        // there are other constraints on the matrix too, see: 
        // https://en.wikipedia.org/wiki/Conjugate_gradient_method#Description_of_the_problem_addressed_by_conjugate_gradients
 
        // this uses the iterative approach (since we're in code and can loop)
        // https://en.wikipedia.org/wiki/Conjugate_gradient_method#As_an_iterative_method
 
        // by going iterative, we recognize that it can fail to converge.
 
        // 1. initial guess
        // we need an initial guess, x0
        // "We denote the initial guess for xâˆ— by x0 (we can assume without loss of generality that x0 = 0"
        // so let x0 = 0
 
        // 2. residual
        // at each step we need to calculate the 'residual', which is
        // b - Ax, where x is the guess, b is what we're trying to sovle for, etc.
        // in examples on wikipedia, we find a basis vector p which is just the
        // residual at a given step like r0 = p0, rk = pk and so on, where k is the
        // kth iteration (or i in the loop)
 
        // 3. subsequent iterations
        // we compute p0 in one way for the first iteration, for subsequent iterations the
        // way we compute it changes:
        // "Since this is the first iteration, we will use the residual vector r0 as our initial search direction p0; the method of selecting pk will change in further iterations."
 
        const { glMatrix } = window;
        const { mat2, vec2 } = glMatrix;
        const DEBUG = false;

        function log(...args) {
            if (!DEBUG) {
                return;
            }
            console.log(...args);
        }

        function logSolution(solution) {
            if (solution) {
                console.log("converged on", solution);
            } else {
                console.log("failed to converge");
            }
        }

        function conjugateGradient2(A, b, iterations = 20) {
            // A = n * n matrix,
            // b is target vector,
            // where we want to solve Ax = b, we will find x.
            log('inputs', A, b);
 
            // as we iterate we'll update this. this is our initial guess above which will be the
            // zero vector
            // let xK = vec2.fromValues(0, 0);
            // TODO
            // only for hte test case
            let xK = vec2.fromValues(0, 0);
 
            // residual at each step.
            let rK = vec2.create();
            let rKNext = vec2.create();
 
            // basis vector that is == residual
            let pK = vec2.create();
 
            // a scalar that is alpha k, changing at each iteration, and used to compute the next rK.
            let alphaK = 0;

            // a scalar that is beta k, changing at each iteration, and used to compute the next pK
            let betaK = 0;
 
            for (let k = 0; k < iterations; k++) {
                log(`=== iteration ${k} ===`);
                const AxK = vec2.create();
                vec2.transformMat2(AxK, xK, A);
                // rkth residual
                vec2.subtract(rK, b, AxK);
                pK = vec2.clone(rK);
                log('residual', rK);
                log('basis vector', pK);

                // compute alpha scalar
                // https://math.stackexchange.com/q/763122 for rK transpose * rK is the same as dot product
                const rKTrK = vec2.dot(rK, rK);
                const ApK = vec2.create();
                vec2.transformMat2(ApK, pK, A);
                const pKTApK = vec2.dot(pK, ApK);
                log('rK transpose times rK', rKTrK);
                log('pK tranpose times A times pK', pKTApK);

                alphaK = rKTrK / pKTApK;
                log('alphaK', alphaK);

                // check if we're within an epsilon of the previous xK.
                // if we're within it, we've gotten close enough to the solution and can break out
                // i.e. we converged.
                const prevxK = vec2.clone(xK);
                vec2.scaleAndAdd(xK, xK, pK, alphaK);
                log('next xK', xK);
                
                if (vec2.equals(prevxK, xK)) {
                    log("=== found a solution (converged) ===")
                    log(xK);
                    return xK;
                }

                vec2.scaleAndAdd(rKNext, rK, ApK, -alphaK);
                log('next rK', rKNext);

                betaK = vec2.dot(rKNext, rKNext) / vec2.dot(rK, rK);
                log('betaK', betaK);

                vec2.scaleAndAdd(pK, rKNext, pK, betaK);
                log('next pK', pK);

                rK = rKNext;
            }

            // failed to converge.
            return null;
       }

       // test case
       // [4, 1]
       // [1, 3]
       // and
       // [1, 2]
       let A = mat2.fromValues(4, 1, 1, 3);
       let b = vec2.fromValues(1, 2);
       logSolution(conjugateGradient2(A, b, 20));

       A = mat2.fromValues(4, 1, 1, 3);
       b = vec2.fromValues(2, 1);
       logSolution(conjugateGradient2(A, b, 20));

       // should fail to converge because
       // [1, 1]
       // [-3, 1]
       // tranpose is not the same as the original matrix!
       A = mat2.fromValues(1, -3, 1, 1);
       b = vec2.fromValues(6, 2);
       logSolution(conjugateGradient2(A, b, 20));
    </script>
</html>